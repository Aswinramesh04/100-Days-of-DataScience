{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNK6W3JWmRFgqMOtUZekNoH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aswinramesh04/100-Days-of-DataScience/blob/main/Day55_Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "                   Day55: Ensemble Learning    By: Loga Aswin"
      ],
      "metadata": {
        "id": "bn8SyMunuDOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble learning**\n",
        "\n",
        "> A machine learning technique that combines predictions from multiple models to improve accuracy.\n",
        "\n",
        "\n",
        "> Aims to mitigate errors or biases that may exist in individual models.\n",
        "\n",
        "\n",
        "> Utilizes the strengths of different models to create a more precise prediction.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IySlxU63OWFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Ensemble Techniques:**\n",
        "\n",
        "\n",
        "> **Max Voting:** The predictions by each model are considered as a 'vote'. The predictions which we get the majority of the models agree on are used as the final prediction.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SUF8hZECWDqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generating some sample data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initializing models\n",
        "model1 = LogisticRegression()\n",
        "model2 = DecisionTreeClassifier()\n",
        "model3 = SVC(probability=True)\n",
        "\n",
        "# Max Voting classifier\n",
        "model = VotingClassifier(estimators=[('lr', model1), ('dt', model2), ('svc', model3)], voting='hard')\n",
        "\n",
        "# Training model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting test results\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Max Voting Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxpkwPY7WPxB",
        "outputId": "77d1e3a5-4c88-4d8b-c719-f9094955eda1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Voting Accuracy: 0.855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> **Averaging**: Averaging aggregates predictions by taking the average probability (for classification) or the mean prediction (for regression) across multiple models.\n",
        "\n",
        "\n",
        "> [We Use probability=True, is used to enable the prediction of probabilities for classes in models that support it, providing more information for soft voting]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ugmqCuVdZ6lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Averaging classifier\n",
        "model = VotingClassifier(estimators=[('lr', model1), ('dt', model2), ('svc', model3)], voting='soft')\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting test result\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Averaging Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMkRARvWaSUq",
        "outputId": "dfde9ce6-b5d7-469f-b66b-0539068fdb7e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Averaging Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Weighted Averaging**: All models are assigned different weights defining the importance of each model for prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "uaHW7WZAed2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define weights for models\n",
        "weights = [0.3, 0.4, 0.3]\n",
        "\n",
        "model = VotingClassifier(estimators=[('lr', model1), ('dt', model2), ('svc', model3)], voting='soft', weights=weights)\n",
        "\n",
        "# Training model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting test results\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Weighted Averaging Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqX9MVg6evmc",
        "outputId": "3b4baca8-2f6b-43c8-9330-ba97d945df55"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted Averaging Accuracy: 0.895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced Ensemble Techniques:**\n",
        "\n",
        "\n",
        "> **Stacking**: A new model is built on the predictions of other models.\n",
        "\n",
        "\n",
        "> **Blending**: A new model is built on the predictions of other models and the actual values of the training set.\n",
        "\n",
        "**Algorithms based on Bagging and Boosting:**\n",
        "\n",
        "> **Bagging**: Multiple subsets are created from the original dataset, selecting observations with replacement. A base model is created on each of these subsets."
      ],
      "metadata": {
        "id": "f0MoQJ3WfgzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn import tree\n",
        "model = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tuo97U86grxF",
        "outputId": "f68a482b-015b-4637-b45d-35ce8ebb63cb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.875"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "model = BaggingRegressor(tree.DecisionTreeRegressor(random_state=1))\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGaNygETiTCs",
        "outputId": "5607cfc6-1e5e-4dff-8891-08524e18430e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6504873882021907"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Boosting**: A sequential process, where each subsequent model attempts to correct the errors of the previous model."
      ],
      "metadata": {
        "id": "CQZ8ZhAZifZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AdaBoost:**\n",
        "\n",
        "\n",
        "> **AdaBoost** (Adaptive Boosting) is an ensemble learning algorithm that combines multiple weak learners to create a strong learner.\n",
        "\n",
        "\n",
        "> It is an iterative algorithm that sequentially builds weak learners, where each weak learner focuses on the hardest examples from the previous round.\n",
        "\n",
        "\n",
        "> AdaBoost is known for its ability to handle noisy data and its robustness to overfitting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ofwOmSVEld1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample code for classification\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "model = AdaBoostClassifier(random_state=1)\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUmR-sNzi2eD",
        "outputId": "9b71ee43-eb50-4ce3-a827-a1dd8c0f5f01"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.87"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample code for regression problem:**"
      ],
      "metadata": {
        "id": "D2y_LU_rl0zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "model = AdaBoostRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJQa0Z4tjMoV",
        "outputId": "14d98074-7549-4c19-cbc5-90113a0f7826"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4132620642489211"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Boosting Machines (GBM)**:\n",
        "\n",
        "\n",
        "\n",
        "> **Gradient Boosting Machines (GBM)** is an ensemble learning algorithm that builds a sequence of weak learners, where each weak learner is trained to minimize the gradient of the loss function with respect to the predictions of the previous weak learner.\n",
        "\n",
        "\n",
        "> GBM is a powerful algorithm that can achieve high accuracy on a variety of tasks.\n"
      ],
      "metadata": {
        "id": "Crx-IYSYmaXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "model= GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVs1ayZHm4oC",
        "outputId": "4e07d322-81d0-456b-c0a1-436919b56891"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.89"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample code for Regressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "model= GradientBoostingRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QOAzcAvnQSq",
        "outputId": "d2fad824-294c-46cd-8ff7-f2f3d91b9f91"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6132034021878043"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBoost:**\n",
        "\n",
        "\n",
        "> **XGBoost** is an optimized version of GBM that includes several improvements, such as:\n",
        "\n",
        "1.   Parallel Processing: XGBoost implements parallel processing and is faster than GBM .\n",
        "2.   Regularization techniques: XGBoost uses regularization techniques to prevent overfitting, which is a common problem in machine learning.\n",
        "\n",
        "[*Since XGBoost takes care of the missing values itself, you do not have to impute the missing values. ]"
      ],
      "metadata": {
        "id": "s2qDcV9gnol3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "model=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiyDgkiLoT9u",
        "outputId": "0f97b048-71b5-4278-bf12-9391b645be2f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.88"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "model=xgb.XGBRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek2h3F2no1YI",
        "outputId": "56074860-971c-42d0-a4c2-6e9ce5540290"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6251452430469582"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LightGBM:**\n",
        "\n",
        "\n",
        "> **LightGBM** is another optimized version of GBM that is known for its speed and efficiency.\n",
        "\n",
        "\n",
        "> It uses a novel tree-growing algorithm that is specifically designed for boosting algorithms.\n",
        "\n",
        "\n",
        "> **LightGBM** also includes several other optimizations that make it faster than XGBoost, such as:\n",
        "\n",
        "\n",
        "\n",
        "1.   Parallel processing: LightGBM can be trained on multiple CPUs or GPUs, which can significantly reduce training time.\n",
        "2.   Histogram-based tree learning: LightGBM uses a histogram-based tree learning algorithm that is faster than traditional tree learning algorithms.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "989ytLaFpahj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "model = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lgb_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"LightGBM Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lSCSHGApH9H",
        "outputId": "8ecd6f06-b7fa-4644-c1d2-11f9f44931df"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 393, number of negative: 407\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000193 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 5100\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491250 -> initscore=-0.035004\n",
            "[LightGBM] [Info] Start training from score -0.035004\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "LightGBM Accuracy: 0.895\n"
          ]
        }
      ]
    }
  ]
}