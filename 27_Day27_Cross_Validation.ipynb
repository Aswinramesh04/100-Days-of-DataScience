{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtcOkt/ScwIQe1LoYRWqsV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aswinramesh04/100-DaysOfCode-DataScience/blob/main/27_Day27_Cross_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "                          Day27: Cross-Validation     By: Loga Aswin"
      ],
      "metadata": {
        "id": "0ZKdNpMEy6LC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Cross-Validation?**\n",
        "\n",
        "> Cross-validation is a technique for evaluating a machine learning model and testing its performance. It is a way to test how good a machine learning model is. It's like giving a test to see if the model can solve real-world problems. CV is commonly used in applied ML tasks.\n",
        "\n",
        "**The Core Algorithm of Cross-Validation**\n",
        "\n",
        "Cross-validation methods share a common algorithmic structure:\n",
        "> **Data Split:** The dataset is divided into two distinct subsetsâ€”one for training and the other for testing.\n",
        "\n",
        "> **Model Training:** The machine learning model is trained on the training dataset.\n",
        "\n",
        "> **Model Validation:** The trained model is then validated using the test dataset.\n",
        "\n",
        "> **Repetitions:** Steps 1 to 3 are repeated a number of times, which depends on the specific cross-validation technique employed."
      ],
      "metadata": {
        "id": "js1WejJakum_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are plenty of CV techniques. Some of them are commonly used:**\n",
        "\n",
        "**1. Hold-out cross-validation:**\n",
        "We simply split the data into two parts.\n",
        "\n",
        "How It Works:\n",
        "\n",
        "**Data Split:** You divide your dataset into two parts: the training set and the test set. Typically, 80% of the data goes into the training set, and 20% goes into the test set, but you can adjust these percentages as needed.\n",
        "\n",
        "**Model Training:** You teach your model on the training set.\n",
        "\n",
        "**Model Testing:** You test your model on the test set.\n",
        "\n",
        "**Result:** You save the outcome of the test. That's it!\n",
        "\n"
      ],
      "metadata": {
        "id": "lXcnHZi4tApr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# sample data\n",
        "data = np.arange(20).reshape((10, 2))\n",
        "labels = np.array([0, 0, 1, 1, 1, 0, 0, 1, 1, 0])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Now X_train and y_train contain 70% of the data for training, and X_test and y_test contain 30% for testing.\n"
      ],
      "metadata": {
        "id": "0ox7qpolvVgH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. k-Fold cross-validation:**\n",
        "We divide the data into 'k' parts and test the model with each part.\n",
        "\n",
        "**The algorithm of the k-Fold technique:**\n",
        "\n",
        "Divide into Folds: Split your data into 'k' equal parts, like dividing a pie into slices.\n",
        "\n",
        "Test One Slice: Take one slice (fold) as a test set and the others as training sets.\n",
        "\n",
        "Train Model: Train a new model using the training slices.\n",
        "\n",
        "Test Model: Test the model on the slice you set aside.\n",
        "\n",
        "Repeat: Do this 'k' times, each time with a different slice as the test set.\n",
        "\n",
        "Average Results: Average the results from all 'k' tests to see how well your model works overall.\n",
        "\n",
        "**In general, it is always better to use k-Fold technique instead of hold-out.**"
      ],
      "metadata": {
        "id": "4b2R2S0awQ1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Leave-one-out cross-validation:**\n",
        "In this method, each data point is used as a testing instance while the rest are used for training.\n",
        "\n",
        "**4. Leave-P-Out:** Similar to Leave-One-Out, but we use 'p' pieces at a time.\n",
        "\n",
        "**5. Stratified K-Folds:** Like K-Folds, but it keeps the same kind of data in each part.\n",
        "\n",
        "**6. Repeated K-Folds:** We repeat K-Folds many times with different data splits.\n",
        "\n",
        "**7. Nested K-Folds:** A combination of K-Folds used for different kinds of tests.\n",
        "\n",
        "**8. Time Series Cross-Validation:** Specifically designed for time series data to account for temporal dependencies."
      ],
      "metadata": {
        "id": "39a2a2wsxREq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-Validation in Machine Learning:**\n",
        "\n",
        "**scikit-learn (sklearn):** This popular Python library provides various tools for machine learning, including functions for easy cross-validation techniques such as k-Fold and stratified sampling.\n",
        "\n",
        "**CatBoost:** CatBoost is a gradient boosting library that supports cross-validation methods to evaluate its models. It has built-in cross-validation functionality.\n",
        "\n",
        "**Cross-Validation in Deep Learning:**\n",
        "\n",
        "**Keras:** Keras is an open-source neural network library that can be used with popular deep learning frameworks like TensorFlow and Theano. It doesn't have specific built-in cross-validation functions, but you can use scikit-learn or other tools for that purpose.\n",
        "\n",
        "**PyTorch:** PyTorch is a deep learning framework with a rich ecosystem of libraries. You can use PyTorch in combination with scikit-learn for cross-validation or implement custom cross-validation techniques.\n",
        "\n",
        "**MxNet (MXNet):** MXNet is another deep learning framework that can be integrated with cross-validation libraries for evaluating deep learning models."
      ],
      "metadata": {
        "id": "fB-lPCXTzEdm"
      }
    }
  ]
}